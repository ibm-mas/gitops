{{- $_job_name_prefix := "installplanpatch" }}

{{- /*
Use the build/bin/set-cli-image-digest.sh script to update this value across all charts.
Included in $_job_hash (see below).
*/}}
{{- $_cli_image_digest := "sha256:934706a7173d45ea53c3d191afa6d75b4e41d25422e6a3685f1c57d1ddf49a00" }}

{{- /*
A dict of values that influence the behaviour of the job in some way.
Any changes to values in this dict will trigger a rerun of the job.
Since jobs must be idemopotent, it's generally safe to pass in values here that are not
strictly necessary (i.e. including some values that don't actually influence job behaviour).
We may want to refine this further though for jobs that can take a long time to complete.
Included in $_job_hash (see below).
*/}}
{{- $_job_config_values := omit .Values "junitreporter" }}

{{- /*
Increment this value whenever you make a change to an immutable field of the Job resource.
E.g. passing in a new environment variable.
Included in $_job_hash (see below).
*/}}
{{- $_job_version := "v3" }}

{{- /*
10 char hash appended to the job name taking into account $_job_config_values, $_job_version and $_cli_image_digest
This is to ensure ArgoCD will create a new job resource intead of attempting (and failing) to update an
immutable field of any existing Job resource.
*/}}
{{- $_job_hash := print ($_job_config_values | toYaml) $_cli_image_digest $_job_version | adler32sum }}

{{- $_job_name := join "-" (list $_job_name_prefix $_job_hash )}}


{{- /*
Set as the value for the mas.ibm.com/job-cleanup-group label on the Job resource.

When the auto_delete flag is not set on the root application, a CronJob in the cluster uses this label 
to identify old Job resources that should be pruned on behalf of ArgoCD.

Any Job resources in the same namespace that have the mas.ibm.com/job-cleanup-group with this value
will be considered to belong to the same cleanup group. All but the most recent (i.e. with the latest "creation_timestamp")
Jobs will be automatically deleted.

$_job_cleanup_group can usually just be based on $_job_name_prefix. There are some special cases
where multiple Jobs are created in our templates using a Helm loop. In those cases, additional descriminators
must be added to $_job_cleanup_group.

By convention, we sha1sum this value to guarantee we never exceed the 63 char limit regardless of which discriminators
are required here.

*/}}
{{- $_job_cleanup_group := cat $_job_name_prefix | sha1sum }}

---
apiVersion: v1
kind: Namespace
metadata:
  name: {{ .Values.odh_namespace }}
  annotations:
    argocd.argoproj.io/sync-wave: "124"
---
apiVersion: v1
kind: ServiceAccount
metadata:
  name: installplan-approver
  namespace: {{ .Values.odh_namespace }}
  annotations:
    argocd.argoproj.io/sync-wave: "125"
---
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: installplan-approver
  namespace: {{ .Values.odh_namespace }}
  annotations:
    argocd.argoproj.io/sync-wave: "126"
rules:
  - apiGroups: ["operators.coreos.com"]
    resources: ["installplans"]
    verbs: ["get", "list", "patch"]
---
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: installplan-approver
  namespace: {{ .Values.odh_namespace }}
  annotations:
    argocd.argoproj.io/sync-wave: "127"
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: installplan-approver
subjects:
  - kind: ServiceAccount
    name: installplan-approver
    namespace: {{ .Values.odh_namespace }}
---
apiVersion: batch/v1
kind: Job
metadata:
  name: {{ $_job_name }}-{{ .Values.odh_operator_version }}
  namespace: {{ .Values.odh_namespace }}
  annotations:
    argocd.argoproj.io/sync-wave: "128"
  labels:
    mas.ibm.com/job-cleanup-group: {{ $_job_cleanup_group }}
spec:
  template:
    spec:
      serviceAccountName: installplan-approver
      containers:
        - name: installplanjob
          image: quay.io/ibmmas/cli@{{ $_cli_image_digest }}
          imagePullPolicy: IfNotPresent
          resources:
            limits:
              cpu: 200m
              memory: 512Mi
            requests:
              cpu: 10m
              memory: 64Mi
          env:
            - name: ODH_NAMESPACE
              value: "{{ .Values.odh_namespace }}"
            - name: ODH_VERSION
              value: "{{ .Values.odh_operator_version }}"
          command:
            - /bin/sh
            - -c
            - |
              echo "Waiting for InstallPlans to be created..."
              sleep 60

              for i in $(seq 1 20); do
                IP=$(oc get installplan -n "${ODH_NAMESPACE}" -o json \
                  | jq -r --arg ODH_VERSION "$ODH_VERSION" '.items[] | select(.spec.clusterServiceVersionNames[] == $ODH_VERSION) | .metadata.name')
                
                if [ "$IP" ]; then
                  echo "Approving InstallPlan for ODH: $IP"
                  oc patch installplan $IP -n ${ODH_NAMESPACE} --type merge --patch '{"spec":{"approved":true}}'
                  break
                fi
                echo "InstallPlan not found. Retry $i..."
                sleep 15
              done
      restartPolicy: OnFailure